# SciQA

**SciQA** [[1]](#myfootnote1)</sup> is scientific QA benchmark for scholarly knowledge. leveraging ORKG.  The dataset os created by manually developed a set of 100 complex questions that are answerable with ORKG. Based on those questions, eight question templates are adopted to automatically generated further 2465 questions. In total the dataset is composed of 2565 questions, along with the SPARQL queries, answers, etc. The dataset is available on [huggingface](:https://huggingface.co/datasets/orkg/SciQA) and [zenodo](https://zenodo.org/doi/10.5281/zenodo.5845197). 


## Leaderboard 


|       Model / System       | Year | Precision | Recall | F1 | Language |         Reported by              |
|:--------------------------:|:----:|:------:|:----:|:----:|:--------:|:---------------------------------------------------------------------:|
|       JarvisQALcs        | 2023 |  0.138  |  0.138  |  0.138  |   EN     | [Auer et al.](https://doi.org/10.1038/s41598-023-33607-z) |



## References
<a name="myfootnote1">[1]</a> Auer, S., Barone, D.A.C., Bartz, C. et al. "The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge". https://doi.org/10.1038/s41598-023-33607-z

[Go back to the README](../README.md)